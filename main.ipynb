{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":["bFQ8FsKNtnrr","NTbyaABttqTU","DF6_OdTnXf3W","-3VBjqnb-JBS","NEWRZ27yARuf","SUTCU3yzAdJT","0MzvdZSU_AWe","iY28vhKQN5M4","p_O5Tw5nhIPI","TcwnFQNa-t_K","UfFzJp30Yp6r","O5CCk-khY2az","z7-bSqpxyl3X"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hzolRRT_ANfT"},"source":["# Path e librerie\n","## Prima di modificare avvertire gli altri!"]},{"cell_type":"markdown","metadata":{"id":"NvLbCq-Etlj-"},"source":["## Install"]},{"cell_type":"code","metadata":{"id":"WkMr6gJYuiBg","tags":[]},"source":["#import drive dir\n","from google.colab import drive\n","drive_dir = 'drive' ; \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0f-_Htz-GUP"},"source":["!pip install polyglot\n","!pip install PyICU\n","!pip install pycld2\n","!pip install morfessor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFQ8FsKNtnrr"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"W2rdoygctsrH"},"source":["import tweepy\n","import numpy as np\n","from tweepy import Stream, OAuthHandler\n","from tqdm.notebook import tqdm\n","# from tqdm import tqdm !! per Jupyter from tqdm.notebook non funziona, runnare questo\n","import pandas as pd\n","pd.set_option('display.max_colwidth', None) # setta il num. di caratteri visibili per ogni cella della colonna\n","# es. 50 = primi 50 caratteri\n","# None = testo completo\n","\n","import io\n","import pprint\n","import os\n","import glob\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from polyglot.detect import Detector\n","import icu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NTbyaABttqTU","tags":[]},"source":["## Path per Colab"]},{"cell_type":"code","metadata":{"id":"eeZCfUCeuglG"},"source":["##########################\n","### ROOT\n","##########################\n","\n","prj_root_dir = f'/content/{drive_dir}/MyDrive/nucleare/'\n","os.makedirs(prj_root_dir, exist_ok=True)\n","\n","##########################\n","### SCRAPING: file dello scraping di ATP\n","##########################\n","\n","scrapingATP_root_dir = prj_root_dir + 'scrapingATP/'\n","os.makedirs(scrapingATP_root_dir, exist_ok=True)\n","\n","##########################\n","### ETL: \n","##########################\n","\n","etl_root_dir = prj_root_dir + 'ETL/'\n","os.makedirs(etl_root_dir, exist_ok=True)\n","\n","##########################\n","### DWH\n","##########################\n","\n","dw_root_dir = prj_root_dir + 'DWH/'\n","os.makedirs(dw_root_dir, exist_ok=True)\n","\n","##########################\n","### SENTIMENT\n","##########################\n","\n","sentiment_root_dir = prj_root_dir + 'SENTIMENT/'\n","os.makedirs(sentiment_root_dir, exist_ok=True)\n","\n","##########################\n","### WORDEMBEDDING\n","##########################\n","\n","embedding_root_dir = prj_root_dir + 'EMBEDDING/'\n","os.makedirs(embedding_root_dir, exist_ok=True)\n","\n","##########################\n","### OUTPUT\n","##########################\n","\n","output_root_dir = prj_root_dir + 'OUTPUT/'\n","os.makedirs(output_root_dir, exist_ok=True)\n","\n","##########################\n","### STAGING ANTONELLO M.\n","##########################\n","\n","st_man_root_dir = prj_root_dir + 'STAGING_MANENTI/'\n","os.makedirs(st_man_root_dir, exist_ok=True)\n","\n","##########################\n","### STAGING ANTONELLO S.\n","##########################\n","\n","tone_root_dir = prj_root_dir + 'tone/'\n","os.makedirs(tone_root_dir, exist_ok=True)\n","\n","\n","##########################\n","### ML MODELS\n","##########################\n","\n","models_root_dir = prj_root_dir + 'MODELS/'\n","os.makedirs(models_root_dir, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"DF6_OdTnXf3W"},"source":["## Path per Jupyter\n","Solo quando si usa Jupyter runnare, il percorso dovrebbe essere lo stesso per tutti/e"]},{"cell_type":"code","metadata":{"id":"5ttVa0GiXf3X"},"source":["##########################\n","### ROOT\n","##########################\n","\n","drive_dir = f'G:\\\\My Drive'\n","prj_root_dir = f'{drive_dir}\\\\nucleare\\\\'\n","\n","##########################\n","### SCRAPING\n","##########################\n","\n","scrapingATP_root_dir = prj_root_dir + 'scrapingATP\\\\'\n","\n","##########################\n","### ETL\n","##########################\n","\n","etl_root_dir = prj_root_dir + 'ETL\\\\'\n","\n","##########################\n","### DWH\n","##########################\n","\n","dw_root_dir = prj_root_dir + 'DWH\\\\'\n","\n","##########################\n","### SENTIMENT\n","##########################\n","\n","sentiment_root_dir = prj_root_dir + 'SENTIMENT\\\\'\n","\n","##########################\n","### WORDEMBEDDING\n","##########################\n","\n","embedding_root_dir = prj_root_dir + 'EMBEDDING\\\\'\n","\n","##########################\n","### OUTPUT\n","##########################\n","\n","output_root_dir = prj_root_dir + 'OUTPUT\\\\'\n","\n","##########################\n","### STAGING ANTONELLO M.\n","##########################\n","\n","st_man_root_dir = prj_root_dir + 'STAGING_MANENTI\\\\'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3VBjqnb-JBS"},"source":["# 1. Ingest\n","## Antonello\n","\n","cartella di scraping: nulceare/scrapingATP"]},{"cell_type":"markdown","metadata":{"id":"NEWRZ27yARuf"},"source":["## Codici Twitter"]},{"cell_type":"code","metadata":{"id":"8MQLVouV-ywL"},"source":["consumer_key = 'MQXChIn29dCky3pOQyW890Uxi'\n","consumer_secret = 'u4n3UeFz58b4H6InWFhBDUPZsfJ8xdVS0wODdnfAjz6e6PUbCJ'\n","access_token = '262247836-oWkm6AZIw4K0Fldnt8VBXwuL9IbtNCZwVj73ODkI'\n","access_secret = 'GySGl7BGaYvmXUtCapu5pHwEDHD5tA3Qd4yCMIDPKm1Wx'\n","auth = OAuthHandler(consumer_key, consumer_secret)\n","auth.set_access_token(access_token, access_secret)\n","api = tweepy.API(auth, wait_on_rate_limit=True)\n","\n","# consumer_key = 'Vzk6O5t967f1n4iz9iQcsQwhf'\n","# consumer_secret = 'Q8sncdY1j85mwNitOqb5Spcg6U6uuZ0E2KNHHZEYF2J4DdCyVi'\n","# access_token = '164224789-mFmU5YhessCNvubNojgfL605tbU0vh0NOna3HdQU'\n","# access_secret = 'Ic5LNOH2QqxziSq4eTQdJvbXo3noTcQDUzCKXgFcdPzpC'\n","# auth = OAuthHandler(consumer_key, consumer_secret)\n","# auth.set_access_token(access_token, access_secret)\n","# api = tweepy.API(auth, wait_on_rate_limit=True)\n","\n","# consumer_key = 'A4GRiWdUDgiGPckQSykzA'\n","# consumer_secret = 'nSyjpFDTr3eEmd8tFeBM1lwVrTNPLaHrcx0DicTEEuw'\n","# access_token = '1678747572-lbkg1jxpe5HypoTRSnRMeiVIe0RBMxhdjLJ0GJ6'\n","# access_secret = '1qVP0pHXV9HDgKhHDIy2HHqqnHFBm3sI5AR2ZDwZxlM'\n","# auth = OAuthHandler(consumer_key, consumer_secret)\n","# auth.set_access_token(access_token, access_secret)\n","# api = tweepy.API(auth, wait_on_rate_limit=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijK4Lyzl-e04"},"source":["utente = 'antonello' #mettere il proprio nome\n","lang = 'all' #mettere la lingua\n","prog = '08' #mettere il progressivo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SUTCU3yzAdJT"},"source":["## Scraping dello storico"]},{"cell_type":"code","metadata":{"id":"67C67NvV-8Da"},"source":["#mettere la query\n","query_00 = ('@janniksin' or '@AlexZverev' or '@DjokerNole' or '@DaniilMedwed' or '@HubertHurkacz' or '@steftsitsipas' or '@CasperRuud98' or '@AndreyRublev97' or 'NittoATPFinals' or 'TennisTakesOverTorino' or '#ATPFinals' or 'ATPFinals' or 'ATP Finals' or 'Djokovic' or 'Medvedev' or 'Zverev' or 'Tsitsipas' or 'Rublev' or 'Berrettini' or 'Hurkacz' or 'Ruud' or 'Sinner')\n","query_01 = ('Zverev' or 'Hurkacz' or '#NittoATPFinals' or 'NittoATPFinals' or 'TennisTakesOverTorino' or '#ATPFinals' or 'ATPFinals' or 'ATP Finals')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2bvjFrw_9F8"},"source":["searched_tweets = []\n","last_id = -1\n","max_tweets = 500_000\n","pbar = tqdm(total=max_tweets)\n","\n","while len(searched_tweets) < max_tweets:\n","    count = max_tweets - len(searched_tweets)\n","    try:\n","        # api.search è un metodo che su colab funziona anche se non è documentato in Tweepy. Su Jupyter non funziona, sostituire con search_tweets\n","        new_tweets = api.search(q=query_01, count=1000, max_id=str(last_id - 1), tweet_mode=\"extended\") # tweet_mode=\"extended\" importante, altrimenti il testo del tweet viene troncato\n","        # If I've reached the end of the search then I'm done\n","        # immediately the cycle while\n","        if not new_tweets:\n","            break\n","        # added the data found to the list\n","        searched_tweets.extend(new_tweets)\n","        # tqdm\n","        pbar.update(len(new_tweets))\n","        # retrieve the last id found\n","        last_id = new_tweets[-1].id\n","    except tweepy.TweepError as e:\n","        print(e)\n","        break\n","\n","pbar.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caBAPyk4AhPM"},"source":["print('Totale Tweet')\n","len(searched_tweets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XTakPozJQN82"},"source":["dict_ = {'id': [], 'user': [], 'date': [], 'text': [], \n","         'favorite_count': [], 'hashtags': [], \n","         'location': [], 'retweet': [], 'retweet_count': [], 'followers_count': [], 'in_reply_to_status_id':[], 'user_mentions':[]}\n","\n","for status_j in searched_tweets[0:max_tweets]:\n","    status = status_j._json\n","\n","    if 'retweeted_status' in status:\n","      if 'extended_tweet' in status['retweeted_status']:\n","        text = status['retweeted_status']['extended_tweet']['full_text']\n","        dict_['text'].append(text)\n","      else:\n","        text = status['retweeted_status']['full_text']\n","        dict_['text'].append(text)\n","    else:\n","      if 'extended_tweet' in status:\n","        text = status['extended_tweet']['full_text']\n","        dict_['text'].append(text)\n","      else:\n","        text = status['full_text']\n","        dict_['text'].append(text)\n","    \n","    dict_['id'].append(status['id'])\n","    dict_['user'].append(status['user']['screen_name'])\n","    dict_['hashtags'].append([hashtag['text'] for hashtag in status['entities']['hashtags']])\n","    dict_['user_mentions'].append([user_mention['screen_name'] for user_mention in status['entities']['user_mentions']])\n","    dict_['location'].append(status['user']['location'])\n","    dict_['followers_count'].append(status['user']['followers_count'])\n","    dict_['in_reply_to_status_id'].append(status['in_reply_to_status_id'])\n","    dict_['date'].append(status['created_at'])\n","    dict_['favorite_count'].append(status['favorite_count'])\n","    dict_['retweet_count'].append(status['retweet_count'])\n","    dict_['retweet'].append(status['retweeted'])\n","\n","df = pd.DataFrame.from_dict(dict_, orient=\"columns\")\n","df = df.set_index(\"id\")\n","df.sort_values(by=['date'], inplace=True, ascending=True)\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEMYyh7EQYiO"},"source":["df = pd.DataFrame.from_dict(dict_, orient=\"columns\")\n","df = df.set_index(\"id\")\n","df.sort_values(by=['date'], inplace=True, ascending=True)\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etTJY7DM_XrJ"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCHC8WmOA98e"},"source":["print(df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFJs4Pl-_KcU"},"source":["df.to_csv(f'{scrapingATP_root_dir}twitter_history_{utente}_{prog}_{lang}_{len(searched_tweets)}.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0MzvdZSU_AWe"},"source":["## Scraping dello stream"]},{"cell_type":"code","metadata":{"id":"eqFqbRuR_lGM"},"source":["class MyStreamListener(tweepy.StreamListener):\n","    data = [] #qui ci metto i tweet che scarico man mano\n","    num = 0 #questo è un contatore settato a 0\n","\n","    def store(self):\n","      dict_ = {'id': [], 'user': [], 'date': [], 'text': [],\n","         'favorite_count': [], 'hashtags': [], \n","         'location': [], 'retweet': [], 'retweet_count': [], 'followers_count': [], 'in_reply_to_status_id':[], 'user_mentions':[]}\n","      \n","      for status_j in self.data:\n","          status = status_j._json\n","\n","          if 'retweeted_status' in status:\n","            if 'extended_tweet' in status['retweeted_status']:\n","              text = status['retweeted_status']['extended_tweet']['full_text']\n","              dict_['text'].append(text)\n","            else:\n","              text = status['retweeted_status']['full_text']\n","              dict_['text'].append(text)\n","          else:\n","            if 'extended_tweet' in status:\n","              text = status['extended_tweet']['full_text']\n","              dict_['text'].append(text)\n","            else:\n","              text = status['full_text']\n","              dict_['text'].append(text)\n","\n","          dict_['id'].append(status['id'])\n","          dict_['user'].append(status['user']['screen_name'])\n","          dict_['hashtags'].append([hashtag['text'] for hashtag in status['entities']['hashtags']])\n","          dict_['user_mentions'].append([user_mention['screen_name'] for user_mention in status['entities']['user_mentions']])\n","          dict_['location'].append(status['user']['location'])\n","          dict_['followers_count'].append(status['user']['followers_count'])\n","          dict_['in_reply_to_status_id'].append(status['in_reply_to_status_id'])\n","          dict_['date'].append(status['created_at'])\n","          dict_['full_text'].append(status['full_text'])\n","          dict_['favorite_count'].append(status['favorite_count'])\n","          dict_['retweet_count'].append(status['retweet_count'])\n","          dict_['retweet'].append(status['retweeted'])\n","\n","      df = pd.DataFrame.from_dict(dict_, orient=\"columns\")\n","      df = df.set_index(\"id\")\n","      df.sort_values(by='favorite_count', inplace=True, ascending=False)\n","      df.to_csv(f'{scrapingATP_root_dir}_twitter_stream_{utente}_{prog}_{lang}_' + str(self.num) + '.csv') #salvo in csv\n","      self.num = self.num + 1\n","      self.data = []\n","\n","\n","    def on_status(self, status):\n","        self.data.append(status)\n","        # to pandas\n","        # store su google drive\n","        if len(self.data) > 5000:\n","          self.store()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7tJCy-B_pAf"},"source":["myStreamListener = MyStreamListener()\n","myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener, tweet_mode=\"extended\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sY7zHFiI_ptN"},"source":["query = ['NittoATPFinals' or '#ATPFinals' or 'ATPFinals' or 'TennisTakesOverTorino' or 'Djokovic' or 'Medvedev' or 'Zverev' or 'Tsitsipas' or 'Rublev' or 'Berrettini' or 'Hurkacz' or 'Ruud' or 'Sinner']\n","myStream.filter(track=query)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvUTo5db_ulg"},"source":["len(myStreamListener.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od_-LHco_yh6"},"source":["dict_ = {'id': [], 'user': [], 'date': [], 'text': [], \n","         'favorite_count': [], 'hashtags': [], \n","         'location': [], 'retweet': [], 'retweet_count': [], 'followers_count': [], 'in_reply_to_status_id':[], 'user_mentions':[]}\n","#\n","for status_j in myStreamListener.data:\n","    status = status_j._json\n","\n","    if 'retweeted_status' in status:\n","      if 'extended_tweet' in status['retweeted_status']:\n","        text = status['retweeted_status']['extended_tweet']['full_text']\n","        dict_['text'].append(text)\n","      else:\n","        text = status['retweeted_status']['text']\n","        dict_['text'].append(text)\n","    else:\n","      if 'extended_tweet' in status:\n","        text = status['extended_tweet']['full_text']\n","        dict_['text'].append(text)\n","      else:\n","        text = status['text']\n","        dict_['text'].append(text)\n","\n","    dict_['id'].append(status['id'])\n","    dict_['user'].append(status['user']['screen_name'])\n","    dict_['hashtags'].append([hashtag['text'] for hashtag in status['entities']['hashtags']])\n","    dict_['user_mentions'].append([user_mention['screen_name'] for user_mention in status['entities']['user_mentions']])\n","    dict_['location'].append(status['user']['location'])\n","    dict_['followers_count'].append(status['user']['followers_count'])\n","    dict_['in_reply_to_status_id'].append(status['in_reply_to_status_id'])\n","    dict_['date'].append(status['created_at'])\n","    dict_['favorite_count'].append(status['favorite_count'])\n","    dict_['retweet_count'].append(status['retweet_count'])\n","    dict_['retweet'].append(status['retweeted'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxsfeH0r_0YX"},"source":["df = pd.DataFrame.from_dict(dict_, orient=\"columns\")\n","df = df.set_index(\"id\")\n","df.sort_values(by='favorite_count', inplace=True, ascending=False)\n","df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6hPQxS3_6fj"},"source":["df.to_csv(f'{scrapingATP_root_dir}twitter_stream_{utente}_{prog}_{lang}_{len(myStreamListener.data)}.csv') #scarico l'ultimo stream"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypONx-rx_99j"},"source":["#unisco tutti i csv\n","\n","path = scrapingATP_root_dir\n","\n","all_files = glob.glob(os.path.join(path, \"twitter_*.csv\"))\n","df_from_each_file = (pd.read_csv(f, sep=',') for f in all_files)\n","df_merged = pd.concat(df_from_each_file, ignore_index=True)\n","df_merged.drop_duplicates(subset=['id'], inplace=True, ignore_index=False)\n","df_merged.to_csv(f'{etl_root_dir}twitter_merged_from_scraping.csv', index=False, sep=',', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5e-bKUNUHtFr"},"source":["df_merge = pd.read_csv(f'{etl_root_dir}twitter_merged_from_scraping.csv')\n","df_merge.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EkyiON6TKi0T"},"source":["df_merge.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iY28vhKQN5M4"},"source":["# 2. ETL\n","**NATALIA**\n","\n","\n","\n","1.   rimuovere #\n","2.   rimuovere id doppi\n","1.   rimuovere http\n","2.   rimuovere twitter con testo vuoto\n","1.   rimuovere @\n","\n","\n","\n","file input: nucleare/ETL/twitter_merged_from_scraping.csv\n","\n","cartella output: nucleare/DWH/"]},{"cell_type":"code","metadata":{"id":"XGHss6_srU9O"},"source":["df = pd.read_csv(etl_root_dir+'twitter_merged_from_scraping.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEPS35oxtHgf"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdaWLYkXrU41"},"source":["df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d28byin7p8vS"},"source":["df.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5TL7ra1tyaP"},"source":["df_clean=df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5jNXbb4zmY7"},"source":["df_clean = df_clean.drop_duplicates(subset='id', keep=\"first\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37XFbKlVt8qv"},"source":["df_clean.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kw84OdNJqCbo"},"source":["df_clean['text_nohash']=df_clean['text']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6F8OQmtb26pG"},"source":["# rimuovo gli hashtag\n","df_clean['text_nohash']=df_clean['text_nohash'].replace('#\\w+ ?','', regex=True)\n","#stringwithouthash = re.sub(r'#\\w+ ?', '', tweet['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgFTJdZJ4ynr"},"source":["# rimuovo le mention\n","df_clean['text_nohash_men_']=df_clean['text_nohash'].replace('@\\w+ ?','', regex=True)\n","#stringwithouthash = re.sub(r'@\\w+ ?', '', tweet['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5yg_xih54eZD"},"source":["# rimuovo i link\n","df_clean['text_nohash_men_http']=df_clean['text_nohash_men_'].replace('http\\S+','', regex=True)\n","#stringwithoutlink = re.sub(r'http\\S+', '', tweet['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-wk0X88vT18"},"source":["#df_clean['text_cleaned'] = df_clean['text_cleaned'].str.replace('\\n','', regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zEz-l2a32TeP"},"source":["df_clean['text_nohash_men_http_nl'] = df_clean['text_nohash_men_http'].apply(lambda x: x.replace('\\n','') + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmyFm0_5l2un"},"source":["#df_clean['text_nohash_men_http_nl'] = df_clean['text_nohash_men_http'].replace(r'\\\\n',' ', regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRTwaPfpvYfb"},"source":["# rimuovo i doppi spazi\n","df_clean['text_nohash_men_http_nl_2spazi'] = df_clean['text_nohash_men_http_nl'].str.replace('\\s+',' ', regex=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhZnABMe5ABF"},"source":["df_clean['text_nohash_men_http_nl_2spazi']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"han5vD3M1LJy"},"source":["#rimuovo i numeri\n","df_clean['text_nohash_men_http_nl_2spazi_num'] = df_clean['text_nohash_men_http_nl_2spazi'].str.replace('\\d+', '')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbH0oQnuvLec"},"source":["# rimuovo le emoticon e la punteggiatura\n","df_clean['text_nohash_men_http_nl_2spazi_num_simboli']=df_clean['text_nohash_men_http_nl_2spazi_num'].str.replace('[^\\w\\s]',' ', regex=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsBTSDHh7IU1"},"source":["df_clean.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIohraCB7NO5"},"source":["df_clean['text_nohash_men_http_nl_2spazi_num_simboli'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUZ1NTtS-CQF"},"source":["df_clean['text_nohash_men_http_nl_2spazi_num_simboli'] = df_clean['text_nohash_men_http_nl_2spazi_num_simboli'].str.replace('\\s+',' ', regex=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkrcp1VL-HNN"},"source":["df_clean['text_nohash_men_http_nl_2spazi_num_simboli'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wV88XWd84_nn"},"source":["df_clean.to_csv(dw_root_dir+'twitter_text_cleaned.csv', index=False, sep=',',encoding='utf-8')\n","#df_clean.to_csv(dw_root_dir+'twitter_text_cleaned.csv')\n","df = pd.read_csv(dw_root_dir+'twitter_text_cleaned.csv', sep=',')\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_O5Tw5nhIPI"},"source":["# 3. Language detection\n","ANTONELLO M."]},{"cell_type":"code","metadata":{"id":"Tq0WsYPT_Dpm"},"source":["df = pd.read_csv(f'{dw_root_dir}twitter_text_cleaned.csv')\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAncVTvN_NeT"},"source":["# testing polyglot\n","df_test = df[['text_cleaned', 'text']]\n","df_test = df_test.sample(n=1, random_state=13)\n","df_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sV7sL1U_PoV"},"source":["df_test['poly_obj'] = df_test.text_cleaned.apply(lambda x: Detector(x, quiet=True))\n","df_test['text_lang'] = df_test.poly_obj.apply(lambda x: icu.Locale.getDisplayName(x.language.locale))\n","df_test['text_lan_code'] = df_test.poly_obj.apply( lambda x: x.language.code)\n","df_test['lang_confidence'] = df_test.poly_obj.apply( lambda x: x.language.confidence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x77RVB9m_RtZ"},"source":["df_test.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"icFl1idO_VcV"},"source":["df_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1tv-5mm_ZBF"},"source":["df_test['poly_obj'] = df_test['poly_obj'].astype(str)\n","df_test_predict = df_test['poly_obj'].str.split(\" \", expand=True)\n","df_test_predict.rename(columns={3:'prediction'}, inplace=True)\n","df_test_predict = df_test_predict.drop([0, 1, 2, 4, 5, 6], axis=1)\n","df_test_predict = df_test_predict.iloc[:,0:1]\n","df_test_predict['prediction_reliable'] = df_test_predict['prediction'].str.replace(r'\\n([-A-Za-z0-9_.]+)', '') \n","df_test_predict = df_test_predict.drop(['prediction'], axis=1)\n","df_test = pd.concat([df_test, df_test_predict], axis=1) \n","df_test = df_test.drop(['poly_obj'], axis=1)\n","df_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZOup7q7_buE"},"source":["# apply polyglot\n","df['poly_obj'] = df.text_cleaned.apply(lambda x: Detector(x, quiet=True))\n","df['text_lang'] = df.poly_obj.apply(lambda x: icu.Locale.getDisplayName(x.language.locale))\n","df['text_lan_code'] = df.poly_obj.apply( lambda x: x.language.code)\n","df['lang_confidence'] = df.poly_obj.apply( lambda x: x.language.confidence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhYcHSeO_eWp"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MfMo8BUD_gO5"},"source":["df['poly_obj'] = df['poly_obj'].astype(str)\n","df_det_lang = df['poly_obj'].str.split(\" \", expand=True)\n","df_det_lang.rename(columns={3:'prediction'}, inplace=True)\n","df_det_lang = df_det_lang.drop([0, 1, 2, 4, 5, 6], axis=1)\n","df_det_lang = df_det_lang.iloc[:,0:1]\n","df_det_lang['prediction_reliable'] = df_det_lang['prediction'].str.replace(r'\\n([-A-Za-z0-9_.]+)', '') \n","df_det_lang = df_det_lang.drop(['prediction'], axis=1)\n","df = pd.concat([df, df_det_lang], axis=1) \n","df = df.drop(['poly_obj'], axis=1)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SblO-QzZhp27"},"source":["df.to_csv(dw_root_dir+'twitter_text_cleaned_lang_det.csv', index=False, sep=',', encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWjLQ8r__839"},"source":["# 4. Player detection\n","ANTONELLO M."]},{"cell_type":"markdown","metadata":{"id":"TcwnFQNa-t_K"},"source":["# 5. Sentiment\n","MARCO"]},{"cell_type":"code","metadata":{"id":"GXEg0QEgvgsH"},"source":["pip install vader-multi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSazrmk4dJA9"},"source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/nucleare/DWH/twitter_text_cleaned.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m4VeaYVgdFHL"},"source":["from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","\n","\n","vader=[]\n","\n","analyzer = SentimentIntensityAnalyzer()\n","\n","for x in (df.loc[0:150,'text_cleaned']):\n","  a = analyzer.polarity_scores(x)\n","  vader.append(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rJaCP_adKPf"},"source":["df1 = pd.DataFrame(vader)\n","df1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VpEWYanodXwe"},"source":["print(df1)\n","df1.to_csv('/content/drive/MyDrive/nucleare/SENTIMENT/prova1.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVdTk8UXxPkN"},"source":["# 6. BoW\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UfFzJp30Yp6r"},"source":["## 6.1 preprocessing our dataset: lematizzation, stopwords, ngrams"]},{"cell_type":"code","metadata":{"id":"e2oGKuQdxPkT"},"source":["import nltk ### libreria di linguistica (Stanford)\n","from nltk.stem import WordNetLemmatizer #### individua la radice linguistica delle parole: non tronca le parole come un stamming\n","nltk.download('wordnet') ## tassonomia lingua inglese\n","nltk.download('stopwords')\n","\n","import warnings\n","from tqdm import tqdm\n","from pprint import pprint\n","from gensim.models.phrases import Phrases\n","from nltk.corpus import stopwords\n","pd.set_option('display.max_colwidth', None)\n","\n","connectors = stopwords.words('english')\n","stop = stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3rLfDa3ArAt"},"source":["file_path = '/content/drive/MyDrive/nucleare/tone/'+'twitter_text_cleaned_lang_det.csv'\n","df = pd.read_csv(file_path, encoding='utf-8')\n","print(df.isna().sum())\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pdWM2QkA4hc"},"source":["df = df[['id', 'user', 'date', 'text_cleaned', 'text_lang', 'text_lan_code', 'lang_confidence', 'prediction_reliable']]\n","print(df.isna().sum())\n","df.info()\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtkYQ0dK6lhp"},"source":["print(f'original len: {len(df)}')\n","\n","dff = df[df['text_lan_code'].isin(['en'])]\n","eng_len = len(dff)\n","print(f'english len: {eng_len}')\n","\n","dff = dff[dff['prediction_reliable'] == True]\n","dff = dff[dff['lang_confidence'] > 96]\n","\n","print(f'filtered len: {len(dff)}')\n","print(f'loss: {eng_len - len(dff)}')\n","\n","print(dff.isna().sum())\n","print(dff.groupby(['text_lan_code', 'prediction_reliable']).size())\n","print(dff.groupby(['text_lan_code', 'lang_confidence']).size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5Ykdc_uxPkU"},"source":["df = dff.copy()\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","raw_text = df[~df['text_cleaned'].isna()]['text_cleaned']\n","print(len(raw_text))\n","\n","####pulizia: sostituisco tutti i caratteri speciali con uno spazio\n","text = raw_text.str.lower().str.replace('\\s+', ' ', regex=True) ## rimuovi spazi multipli\n","print(len(text))\n","print(text.head())\n","\n","###TOKENIZATION\n","df['token'] = text.str.split() ##text: series of list, text[0] list of string, text[0][0] string \n","\n","###LEMMATIZATION\n","df['ltoken'] = df.token.apply(lambda x: [lemmatizer.lemmatize(sent) for sent in x]) #text: series of list, text[0] list of string (lemmatized), text[0][0] string\n","\n","# print(f'text[0][0]-({type(df.ltoken[0][0])})')\n","# print(f'text[0]-({type(df.ltoken[0])})')\n","# print(f'text-({type(df.ltoken)})')\n","\n","print(f'text_cleaned: {df.text_cleaned[0]}')\n","print(f'token       : {df.token[0]}')\n","print(f'ltoken      : {df.ltoken[0]}')\n","\n","# stop.extend(['good', 'bad', 'dont', 'many', 'love', 'excellent', 'would', 'perfect', 'even', 'great','nice', 'amazing'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ix4tTrNEFTt2"},"source":["number = 3\n","print(f'text_cleaned: {df.text_cleaned.to_list()[number]}')\n","print(f'token       : {df.token.to_list()[number]}')\n","print(f'ltoken      : {df.ltoken.to_list()[number]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UCRsvA9eFynK"},"source":["print(df.isna().sum())\n","len_target = 10\n","field_name ='ltoken' \n","counter = df[df[field_name].str.len() < len_target ][field_name].count()\n","print(f'{field_name} with len < {len_target}: {counter}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kuWZ87umDwBy"},"source":["my_stop_words = []\n","stop.extend(my_stop_words)\n","\n","df['ltoken'] = df.ltoken.apply(lambda row: [item for item in row if item not in my_stop_words])\n","\n","df['stopwords'] = df.ltoken.apply(lambda row: [item for item in row if item in stop])\n","df['nostopwords'] = df.ltoken.apply(lambda row: [item for item in row if item not in stop])\n","\n","bigram  = Phrases(df.ltoken, min_count=5, threshold=0.2, common_terms=connectors) #gensim 3, ad esempio \"and\" \n","df['bigrams_afternostop'] = df.ltoken.apply(lambda row: bigram[row])\n","\n","ngram   = Phrases(df.bigrams_afternostop, min_count=5, threshold=0.2, common_terms=connectors) # gensim 3\n","df['ngrams_afternostop'] = df.bigrams_afternostop.apply(lambda row: ngram[row])\n","\n","df.bigrams_afternostop = df.bigrams_afternostop.apply(lambda bigr: [item for item in bigr if item not in stop])\n","df.ngrams_afternostop = df.ngrams_afternostop.apply(lambda ngr: [item for item in ngr if item not in stop])\n","\n","bigram  = Phrases(df.nostopwords, min_count=5, threshold=0.2, common_terms=connectors) #gensim 3, ad esempio \"and\" \n","df['bigrams_beforenostop'] = df.nostopwords.apply(lambda row: bigram[row])\n","\n","ngram   = Phrases(df.bigrams_beforenostop, min_count=5, threshold=0.2, common_terms=connectors) # gensim 3\n","df['ngrams_beforenostop'] = df.bigrams_beforenostop.apply(lambda row: ngram[row])\n","\n","df['bigrams_afternostop_sent'] = df.bigrams_afternostop.apply(lambda row: ' '.join(row))\n","df['bigrams_beforenostop_sent'] = df.bigrams_beforenostop.apply(lambda row: ' '.join(row))\n","df['ngrams_afternostop_sent'] = df.ngrams_afternostop.apply(lambda row: ' '.join(row))\n","df['ngrams_beforenostop_sent'] = df.ngrams_beforenostop.apply(lambda row: ' '.join(row))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xTAXVZfIJ6W"},"source":["# print(df.isna().sum())\n","len_target = 1\n","# 'bigrams_afternostop_sent' \n","# 'bigrams_beforenostop_sent'\n","# 'ngrams_afternostop_sent'  \n","# 'ngrams_beforenostop_sent' \n","field_name ='ngrams_beforenostop' \n","counter = df[df[field_name].str.len() < len_target ][field_name].count()\n","print(f'{field_name} with len < {len_target}: {counter}')\n","\n","n = 10\n","print(f'df.bigrams_afternostop_sent[{n}] : {df.bigrams_afternostop_sent.to_list()[n]}')\n","print(f'df.bigrams_beforenostop_sent[{n}]: {df.bigrams_beforenostop_sent.to_list()[n]}')\n","print(f'df.ngrams_afternostop_sent[{n}]  : {df.ngrams_afternostop_sent.to_list()[n]}')\n","print(f'df.ngrams_beforenostop_sent[{n}] : {df.ngrams_beforenostop_sent.to_list()[n]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O6DbE5TmJG0N"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tznsaSoSrrWi"},"source":["### 6.1.1 Saving"]},{"cell_type":"code","metadata":{"id":"GYE_PmU-rxVD"},"source":["df_saving = df.copy() #.replace('\\n','', regex=True)\n","df_saving = df_saving[['id', 'user', 'date', 'text_cleaned', 'text_lang', 'text_lan_code',\n","       'lang_confidence', 'prediction_reliable', 'token', 'ltoken',\n","       'stopwords', 'nostopwords', 'bigrams_afternostop', 'ngrams_afternostop',\n","       'bigrams_beforenostop', 'ngrams_beforenostop',\n","       'bigrams_afternostop_sent', 'bigrams_beforenostop_sent',\n","       'ngrams_afternostop_sent', 'ngrams_beforenostop_sent']]\n","saving_file_name = f'{sentiment_root_dir}ngrams_bow_dataframe.csv'\n","df_saving.to_csv(saving_file_name, index=False, encoding='utf-8', sep =',' )\n","df_saving.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5CCk-khY2az"},"source":["## 6.2 Preprocesing Trustpilot dataset"]},{"cell_type":"code","metadata":{"id":"56OEm9zl3ZDq"},"source":["pd.set_option('display.max_colwidth', None)\n","file_path = tone_root_dir +'trustpilot_review_for_ML.csv'\n","df_open = pd.read_csv(file_path)\n","len(df_open)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogYd7hK94JmL"},"source":["\n","dftp = df_open.copy()\n","\n","dftp = dftp[dftp.rating != 'rating']\n","dftp = dftp[~dftp.rating.isin([3,'3', 'rating'])]\n","dftp = dftp.drop_duplicates()\n","dftp.rating = dftp.rating.astype('int') \n","print(len(dftp))\n","print(dftp.groupby(['rating']).size().index)\n","print(dftp.groupby(['rating']).size())\n","\n","df1 = dftp[dftp.rating == 1 ]\n","df2 = dftp[dftp.rating == 2 ]\n","df4 = dftp[dftp.rating == 4 ]\n","df5 = dftp[dftp.rating == 5 ]\n","\n","df4 = df4.sample(len(df2), random_state= 123)\n","df5 = df5.sample(len(df1), random_state= 123)\n","print(f'df1: {len(df1)}')\n","print(f'df2: {len(df2)}')\n","print(f'df4: {len(df4)}')\n","print(f'df5: {len(df5)}')\n","\n","dftp = df1.copy()\n","dftp = dftp.append(df2)\n","dftp = dftp.append(df4)\n","dftp = dftp.append(df5)\n","print(dftp.groupby(['rating']).size())\n","print(dftp.groupby(['rating']).size())\n","print(len(dftp))\n","\n","dftp.to_csv(tone_root_dir +'balanced_trustpilot_review_for_ML.csv', index=False, encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cr3ull-8RJz"},"source":["df_open = pd.read_csv(tone_root_dir +'balanced_trustpilot_review_for_ML.csv', encoding='utf-8')\n","print(df_open.groupby(['rating']).size())\n","print(len(df_open))\n","df_open.rating.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNsRBIo96ZKK"},"source":["df = df_open.copy()\n","print(df.isna().sum())\n","df.rating.value_counts()\n","\n","def binary(row):\n","    if row['rating'] > 3:\n","        val = 1\n","    else:\n","        val = 0\n","    return val\n","\n","df['y'] = df.apply(binary, axis=1) ### axis = 1 --> sulle righe\n","print(len(df))\n","#escludo dal dataset le review null e le review neutre \n","df = df[~(df['comment'].isna())]\n","print(len(df))\n","#recupero dal dataset i testi delle reviews (X)\n","raw_text = df['comment']\n","\n","print(raw_text[1])\n","#recupero la lista dei binary weight (y)\n","y = df['y'].tolist()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nPxvMUMBie1"},"source":["import nltk ### libreria di linguistica (Stanford)\n","nltk.download('wordnet') ## tassonomia lingua inglese\n","\n","from tqdm import tqdm\n","from nltk.stem import WordNetLemmatizer #### individua la radice linguistica delle parole: non tronca le parole come un stamming\n","lemmatizer = WordNetLemmatizer()\n","\n","raw_text = df[~df['comment'].isna()]['comment']\n","print(len(raw_text))\n","\n","####pulizia: sostituisco tutti i caratteri speciali con uno spazio\n","text = raw_text.str.lower().str.replace('[^\\w\\s\\d]',' ', regex=True) #type(text) series of string\n","text = raw_text.str.lower().str.replace('\\s+', ' ', regex=True) ## rimuovi spazi multipli\n","text = raw_text.str.replace('|!\"%&()?^,.-;:_', ' ', regex=True) ## rimuovi spazi multipli\n","print(len(text))\n","print(text.head())\n","\n","###TOKENIZATION\n","df['token'] = text.str.split() ##text: series of list, text[0] list of string, text[0][0] string \n","\n","###LEMMATIZATION\n","df['ltoken'] = df.token.apply(lambda x: [lemmatizer.lemmatize(sent) for sent in x]) #text: series of list, text[0] list of string (lemmatized), text[0][0] string\n","\n","# print(f'text[0][0]-({type(df.ltoken[0][0])})')\n","# print(f'text[0]-({type(df.ltoken[0])})')\n","# print(f'text-({type(df.ltoken)})')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llZq_AUED78d"},"source":["stop.extend(['good', 'bad', 'dont', 'many', 'love', 'excellent', 'would', 'perfect', 'even', 'great','nice', 'amazing'])\n","number = 3\n","print(f'comment     : {df.comment.to_list()[number]}')\n","print(f'token       : {df.token.to_list()[number]}')\n","print(f'ltoken      : {df.ltoken.to_list()[number]}')\n","\n","print(df.isna().sum())\n","\n","len_target = 1\n","field_name ='ltoken' \n","counter = df[df[field_name].str.len() < len_target ][field_name].count()\n","print(f'{field_name} with len < {len_target}: {counter}')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1icd1qUqEISr"},"source":["my_stop_words = []\n","stop.extend(my_stop_words)\n","\n","df['ltoken'] = df.ltoken.apply(lambda row: [item for item in row if item not in my_stop_words])\n","\n","df['stopwords'] = df.ltoken.apply(lambda row: [item for item in row if item in stop])\n","df['nostopwords'] = df.ltoken.apply(lambda row: [item for item in row if item not in stop])\n","\n","df.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dItqZS9DJkVU"},"source":["\n","bigram  = Phrases(df.ltoken, min_count=5, threshold=0.2, common_terms=connectors) #gensim 3, ad esempio \"and\" \n","df['bigrams_afternostop'] = df.ltoken.apply(lambda row: bigram[row])\n","\n","ngram   = Phrases(df.bigrams_afternostop, min_count=5, threshold=0.2, common_terms=connectors) # gensim 3\n","df['ngrams_afternostop'] = df.bigrams_afternostop.apply(lambda row: ngram[row])\n","\n","df.bigrams_afternostop = df.bigrams_afternostop.apply(lambda bigr: [item for item in bigr if item not in stop])\n","df.ngrams_afternostop = df.ngrams_afternostop.apply(lambda ngr: [item for item in ngr if item not in stop])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AezIVSZzKkN1"},"source":["bigram  = Phrases(df.nostopwords, min_count=20, threshold=0.2, common_terms=connectors) #gensim 3, ad esempio \"and\" \n","df['bigrams_beforenostop'] = df.nostopwords.apply(lambda row: bigram[row])\n","\n","ngram   = Phrases(df.bigrams_beforenostop, min_count=20, threshold=0.2, common_terms=connectors) # gensim 3\n","df['ngrams_beforenostop'] = df.bigrams_beforenostop.apply(lambda row: ngram[row])\n","\n","df['bigrams_afternostop_sent'] = df.bigrams_afternostop.apply(lambda row: ' '.join(row))\n","df['bigrams_beforenostop_sent'] = df.bigrams_beforenostop.apply(lambda row: ' '.join(row))\n","df['ngrams_afternostop_sent'] = df.ngrams_afternostop.apply(lambda row: ' '.join(row))\n","df['ngrams_beforenostop_sent'] = df.ngrams_beforenostop.apply(lambda row: ' '.join(row))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ku8RhMvSx2u"},"source":["\n","df.bigrams_afternostop_sent = df.bigrams_afternostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","df.bigrams_beforenostop_sent = df.bigrams_beforenostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","df.ngrams_afternostop_sent = df.ngrams_afternostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","df.ngrams_beforenostop_sent = df.ngrams_beforenostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","\n","print(len(df))\n","df = df[~(df.bigrams_afternostop_sent.str.len() == 0)]\n","df = df[~(df.bigrams_beforenostop_sent.str.len() == 0)]\n","df = df[~(df.ngrams_afternostop_sent.str.len() == 0)]\n","df = df[~(df.ngrams_beforenostop_sent.str.len() == 0)]\n","print(len(df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaVoykP4ENSz"},"source":["\n","\n","# print(df.isna().sum())\n","len_target = 1\n","for field_name in ['token', 'ltoken'\n","              ,'bigrams_afternostop_sent' \n","              ,'bigrams_beforenostop_sent'\n","              ,'ngrams_afternostop_sent'  \n","              ,'ngrams_beforenostop_sent']: \n","  counter = df[df[field_name].str.len() < len_target][field_name].count()\n","  print(f'{field_name} with len < {len_target}: {counter}')\n","\n","n = 10\n","print(f'df.bigrams_afternostop_sent[{n}] : {df.bigrams_afternostop_sent.to_list()[n]}')\n","print(f'df.bigrams_beforenostop_sent[{n}]: {df.bigrams_beforenostop_sent.to_list()[n]}')\n","print(f'df.ngrams_afternostop_sent[{n}]  : {df.ngrams_afternostop_sent.to_list()[n]}')\n","print(f'df.ngrams_beforenostop_sent[{n}] : {df.ngrams_beforenostop_sent.to_list()[n]}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcfSpnliM0Dt"},"source":["df_saving = df.copy()\n","df_saving = df_saving.loc[:, ['comment', 'rating', \n","                      'bigrams_afternostop_sent',\n","                      'bigrams_beforenostop_sent', \n","                      'ngrams_afternostop_sent',\n","                      'ngrams_beforenostop_sent'\n","                      ,'y']]\n","saving_file_name = f'{tone_root_dir}trustpilot_ready4_bow.csv'\n","df_saving.to_csv(saving_file_name, index=False, encoding='utf-8', sep =',' )\n","df_saving.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"avG5NFW_PaG-"},"source":["## 6.3 BOW"]},{"cell_type":"code","metadata":{"id":"UGQ3bcvQNgA2"},"source":["df_open = pd.read_csv(f'{tone_root_dir}trustpilot_ready4_bow.csv', encoding='utf-8', sep =',', header=0 )\n","\n","# df.bigrams_afternostop_sent = df.bigrams_afternostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","# df.bigrams_beforenostop_sent = df.bigrams_beforenostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","# df.ngrams_afternostop_sent = df.ngrams_afternostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","# df.ngrams_beforenostop_sent = df.ngrams_beforenostop_sent.str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ', regex=True).str.strip()\n","\n","# print(len(df_open))\n","# df_open = df_open[~(df_open.bigrams_afternostop_sent.str.len() == 0)]\n","# df_open = df_open[~(df_open.bigrams_beforenostop_sent.str.len() == 0)]\n","# df_open = df_open[~(df_open.ngrams_afternostop_sent.str.len() == 0)]\n","# df_open = df_open[~(df_open.ngrams_beforenostop_sent.str.len() == 0)]\n","# print(len(df_open))\n","\n","print(df_open.isna().sum())\n","df_open.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w58Xsm58Pfok"},"source":["import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","df = df_open.copy()\n","number_of_row = 1000\n","# number_of_row = len(df['y'] == 1])\n","df = df[df['y'] == 1].sample(number_of_row, random_state= 123).append(df[df['y'] == 0].sample(number_of_row, random_state= 123))\n","print(f'reduced df len: {len(df)}')\n","# 'comment', 'rating', 'bigrams_afternostop_sent',\n","#        'bigrams_beforenostop_sent', 'ngrams_afternostop_sent',\n","#        'ngrams_beforenostop_sent', 'y'\n","\n","choosen_column_2vec = 'ngrams_afternostop_sent'\n","\n","vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=1000)\n","# vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000)\n","X = vectorizer.fit_transform(df['ngrams_afternostop_sent'].to_list())\n","# feature_names = vectorizer.get_feature_names()\n","# print(feature_names)\n","\n","X = X.toarray()\n","X = np.array(X)\n","y = np.array(df['y'].to_list())\n","\n","x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTLDlWeGbVDQ"},"source":["all_ngrams = []\n","for ngram_list in df.ngrams_afternostop_sent.to_list():\n","  all_ngrams.extend(ngram_list.split())\n","\n","from collections import Counter\n","counter = Counter(all_ngrams)\n","counter.most_common(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igbKAnS0VhMm"},"source":["from tqdm import tqdm\n","from sklearn import tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.metrics import accuracy_score, recall_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","import pickle\n","\n","#################################################################################à\n","tree_model = tree.DecisionTreeClassifier(max_leaf_nodes=10, max_depth=5)\n","#################################################################################à\n","tree_model.fit(x_train, y_train)\n","\n","# Save to file in the current working directory\n","model_filename = f\"{models_root_dir}basic_tree_model.pkl\"\n","with open(model_filename, 'wb') as file:\n","    pickle.dump(tree_model, file)\n","# Load from file\n","with open(model_filename, 'rb') as file:\n","    tree_model = pickle.load(file)\n","\n","predicted = tree_model.predict(x_test)\n","print(f'tree_model: {classification_report(y_test, predicted)}')\n","\n","#################################################################################à\n","logit_model = LogisticRegression(class_weight=None)\n","#################################################################################à\n","logit_model.fit(x_train, y_train)\n","\n","# Save to file in the current working directory\n","model_filename = f\"{models_root_dir}basic_logit_model.pkl\"\n","with open(model_filename, 'wb') as file:\n","    pickle.dump(logit_model, file)\n","# Load from file\n","with open(model_filename, 'rb') as file:\n","    logit_model = pickle.load(file)\n","\n","predicted = logit_model.predict(x_test)\n","print(f'logit_model: {classification_report(y_test, predicted)}')\n","\n","\n","#################################################################################à\n","random_model = RandomForestClassifier()\n","#################################################################################à\n","random_model.fit(x_train, y_train)\n","\n","# Save to file in the current working directory\n","model_filename = f\"{models_root_dir}basic_random_model.pkl\"\n","with open(model_filename, 'wb') as file:\n","    pickle.dump(random_model, file)\n","# Load from file\n","with open(model_filename, 'rb') as file:\n","    random_model = pickle.load(file)\n","\n","predicted = random_model.predict(x_test)\n","print(f'random_model: {classification_report(y_test, predicted)}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7-bSqpxyl3X"},"source":["# 7. Word embeddings\n","TONE'"]},{"cell_type":"code","metadata":{"id":"jpxFJs7uytJe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lfMGMxdypst"},"source":["# 8. Lime"]},{"cell_type":"code","metadata":{"id":"eOBsslToysP2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyhEC_7ShQUW"},"source":["# 9. SocialNetwork Analisys"]}]}